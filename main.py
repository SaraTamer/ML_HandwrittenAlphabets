# -*- coding: utf-8 -*-
"""S1_S2_Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1P8u72DHw6gkhyEZZJ6l6UVy0d_p_x7Ja
"""

import pandas
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
import tensorflow as tf
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense, Flatten
from sklearn.metrics import confusion_matrix, classification_report, f1_score


# Start with running cell of loading data if error occur start with the following cell


# Load the dataset
file_path = "A_Z Handwritten Data.csv"  # Update the file path
data = pandas.read_csv(file_path)

# # Work on a random subset of 1000 rows
# data = data.sample(n=20000, random_state=42)

# Identify the number of unique classes
n_classes = data.loc[:, '0'].unique().size
print("Number of unique classes:", n_classes)

# show their distribution
# Todo

X = data.drop(columns=['0'])
y = data['0']

# o Normalize each image.
# x(i) = x(i) - x(min) / x(max) - x(min)
# x(min) = 0, x(max) = 255
X = X / 255

# ● Experiments and results:
#   o Split the data into training and testing datasets
#   o First experiment (You can use scikit-learn):
#     ▪ Train 2 SVM models with linear and nonlinear kernels.
#     ▪ Test the models and provide the confusion matrix and the average f-1
#       scores for the testing dataset.

# Split the data into training and testing datasets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# o Reshape the flattened vectors to reconstruct and display the corresponding
# images while testing the models. [From requirement 1]
# each image is converted from [1D] 784 pixels into [2D] 28 x 28 pixels
# dataset is reshaped from 2D to 3D
X_test_reshaped = X_test.to_numpy().reshape(-1, 28, 28)

# SVM
# linear kernels

# from sklearn.svm import LinearSVC
# # training
# linear_svm = LinearSVC(random_state=0)
# linear_svm.fit(X_train, y_train)

# y_pred = linear_svm.predict(X_test)

# print(y_pred[2], y_test.iloc[2])

# # Create image for the 2D array
# plt.imshow(X_test_reshaped[2],cmap = 'gray')

# To save the trained model
# import joblib
# joblib.dump(linear_svm, 'linear_svm_model.pkl')

# Download it to your machine
# from google.colab import files
# files.download('linear_svm_model.pkl')

# SVM
# non-linear kernels

#   o Split the training dataset into training and validation datasets.
#   o Second experiment (Build from scratch):
#     ▪ Implement logistic regression for one-versus-all multi-class
#       classification.
#     ▪ Train the model and plot the error and accuracy curves for the training
#       and validation data.
#     ▪ Test the model and provide the confusion matrix and the average f-1
#       scores for the testing dataset.

# o Third experiment (You can use TensorFlow):
#   ▪ Design 2 Neural Networks (with different number of hidden layers,
#     neurons, activations, etc.)
#   ▪ Train each one of these models and plot the error and accuracy curves
#     for the training data and validation datasets.
#   ▪ Save the best model in a separated file, then reload it.
#   ▪ Test the best model and provide the confusion matrix and the average
#     f-1 scores for the testing data.
#   ▪ Test the best model with images representing the alphabetical letters
#     for the names of each member of your team.
#  o Compare the results of the models and suggest the best model.


# Split the data into training and validation datasets
X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, test_size=0.2, random_state=0)
# Reshape X_train, X_test
X_train_reshaped = X_train.to_numpy().reshape(-1, 28, 28)
X_validation_reshaped = X_validation.to_numpy().reshape(-1, 28, 28)


# Simple example of a neural network structure
model1 = Sequential([
    Flatten(input_shape=(28, 28)),
    Dense(128, activation='relu'),  # Hidden layer with 128 neurons
    Dense(26, activation='softmax') # Output layer (26 classes for A-Z)
])

# Preparing the model by specifying how it will learn and evaluate its performance
model1.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Train the model
history1 = model1.fit(X_train_reshaped, y_train,validation_data = (X_validation_reshaped,y_validation), epochs=10, batch_size=32)


# Simple example of a neural network structure
model2 = Sequential([
    Flatten(input_shape=(28, 28)),
    Dense(256, activation='relu'), 
    Dense(128, activation='tanh'),
    Dense(64, activation='sigmoid'),
    Dense(26, activation='softmax') # Output layer (26 classes for A-Z)
])

# Preparing the model by specifying how it will learn and evaluate its performance
model2.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Train the model
history2 = model2.fit(X_train_reshaped, y_train,validation_data = (X_validation_reshaped,y_validation), epochs=10, batch_size=32)


# Evaluate both models on the test dataset
test_loss1, test_acc1 = model1.evaluate(X_test_reshaped, y_test, verbose=2)
test_loss2, test_acc2 = model2.evaluate(X_test_reshaped, y_test, verbose=2)

# Compare the test accuracies and save the best model
if test_acc1 > test_acc2:
    print(f"Model 1 is better with accuracy: {test_acc1:.4f}")
    model1.save("best_model.h5")
else:
    print(f"Model 2 is better with accuracy: {test_acc2:.4f}")
    model2.save("best_model.h5")


# Reload the best model

# Reload and test the best model1
model1 = tf.keras.models.load_model("best_model.h5")
test_loss1, test_acc1 = model1.evaluate(X_test_reshaped, y_test, verbose=2)
print(f"Best Model Test Accuracy: {test_acc1:.4f}")



def plot_curves(history, model_name):
    # Accuracy curves
    plt.figure(figsize=(12, 4))
    plt.subplot(1, 2, 1)
    plt.plot(history.history['accuracy'], label='Train Accuracy')
    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
    plt.title(f'{model_name} Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()

    # Loss curves
    plt.subplot(1, 2, 2)
    plt.plot(history.history['loss'], label='Train Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.title(f'{model_name} Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()

    plt.show()

plot_curves(history1, "Model 1")
plot_curves(history2, "Model 2")

